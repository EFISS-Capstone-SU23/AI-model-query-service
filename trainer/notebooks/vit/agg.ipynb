{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/saplab/thaiminhpv/EFISS/AI-model-query-service\n"
     ]
    }
   ],
   "source": [
    "%cd ../../.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTImageProcessor, ViTForImageClassification, ViTFeatureExtractor\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import faiss\n",
    "\n",
    "processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224')\n",
    "model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224')\n",
    "model.classifier = nn.Identity()\n",
    "import torch\n",
    "import numpy as np\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "import cv2\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from tqdm import tqdm\n",
    "\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "import pandas as pd\n",
    "df = pd.read_csv('data/shopee_crop_yolo/database.txt', sep=' ', nrows=None, names=['img_path', 'label']).drop('label', axis=1)\n",
    "df\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/34 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 2/34 [00:00<00:01, 19.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 0\n",
      "/media/saplab/MinhNVMe/relahash/temp/embeddings_0.pt\n",
      "Processing chunk 40000\n",
      "/media/saplab/MinhNVMe/relahash/temp/embeddings_40000.pt\n",
      "Processing chunk 80000\n",
      "/media/saplab/MinhNVMe/relahash/temp/embeddings_80000.pt\n",
      "Processing chunk 120000\n",
      "/media/saplab/MinhNVMe/relahash/temp/embeddings_120000.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 6/34 [00:00<00:01, 17.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 160000\n",
      "/media/saplab/MinhNVMe/relahash/temp/embeddings_160000.pt\n",
      "Processing chunk 200000\n",
      "/media/saplab/MinhNVMe/relahash/temp/embeddings_200000.pt\n",
      "Processing chunk 240000\n",
      "/media/saplab/MinhNVMe/relahash/temp/embeddings_240000.pt\n",
      "Processing chunk 280000\n",
      "/media/saplab/MinhNVMe/relahash/temp/embeddings_280000.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 10/34 [00:00<00:01, 16.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 320000\n",
      "/media/saplab/MinhNVMe/relahash/temp/embeddings_320000.pt\n",
      "Processing chunk 360000\n",
      "/media/saplab/MinhNVMe/relahash/temp/embeddings_360000.pt\n",
      "Processing chunk 400000\n",
      "/media/saplab/MinhNVMe/relahash/temp/embeddings_400000.pt\n",
      "Processing chunk 440000\n",
      "/media/saplab/MinhNVMe/relahash/temp/embeddings_440000.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 14/34 [00:00<00:01, 16.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 480000\n",
      "/media/saplab/MinhNVMe/relahash/temp/embeddings_480000.pt\n",
      "Processing chunk 520000\n",
      "/media/saplab/MinhNVMe/relahash/temp/embeddings_520000.pt\n",
      "Processing chunk 560000\n",
      "/media/saplab/MinhNVMe/relahash/temp/embeddings_560000.pt\n",
      "Processing chunk 600000\n",
      "/media/saplab/MinhNVMe/relahash/temp/embeddings_600000.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 18/34 [00:01<00:00, 16.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 640000\n",
      "/media/saplab/MinhNVMe/relahash/temp/embeddings_640000.pt\n",
      "Processing chunk 680000\n",
      "/media/saplab/MinhNVMe/relahash/temp/embeddings_680000.pt\n",
      "Processing chunk 720000\n",
      "/media/saplab/MinhNVMe/relahash/temp/embeddings_720000.pt\n",
      "Processing chunk 760000\n",
      "/media/saplab/MinhNVMe/relahash/temp/embeddings_760000.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▍   | 22/34 [00:01<00:00, 16.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 800000\n",
      "/media/saplab/MinhNVMe/relahash/temp/embeddings_800000.pt\n",
      "Processing chunk 840000\n",
      "/media/saplab/MinhNVMe/relahash/temp/embeddings_840000.pt\n",
      "Processing chunk 880000\n",
      "/media/saplab/MinhNVMe/relahash/temp/embeddings_880000.pt\n",
      "Processing chunk 920000\n",
      "/media/saplab/MinhNVMe/relahash/temp/embeddings_920000.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▋  | 26/34 [00:01<00:00, 15.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 960000\n",
      "/media/saplab/MinhNVMe/relahash/temp/embeddings_960000.pt\n",
      "Processing chunk 1000000\n",
      "/media/saplab/MinhNVMe/relahash/temp/embeddings_1000000.pt\n",
      "Processing chunk 1040000\n",
      "/media/saplab/MinhNVMe/relahash/temp/embeddings_1040000.pt\n",
      "Processing chunk 1080000\n",
      "/media/saplab/MinhNVMe/relahash/temp/embeddings_1080000.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 30/34 [00:01<00:00, 15.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 1120000\n",
      "/media/saplab/MinhNVMe/relahash/temp/embeddings_1120000.pt\n",
      "Processing chunk 1160000\n",
      "/media/saplab/MinhNVMe/relahash/temp/embeddings_1160000.pt\n",
      "Processing chunk 1200000\n",
      "/media/saplab/MinhNVMe/relahash/temp/embeddings_1200000.pt\n",
      "Processing chunk 1240000\n",
      "/media/saplab/MinhNVMe/relahash/temp/embeddings_1240000.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34/34 [00:02<00:00, 16.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 1280000\n",
      "/media/saplab/MinhNVMe/relahash/temp/embeddings_1280000.pt\n",
      "Processing chunk 1320000\n",
      "/media/saplab/MinhNVMe/relahash/temp/embeddings_1320000.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "chunk_size = 40000\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    img = cv2.imread(examples[\"img_path\"])\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    inputs = processor(images=img, return_tensors=\"pt\")\n",
    "    inputs['pixel_values'] = inputs['pixel_values'].squeeze()\n",
    "    return inputs\n",
    "\n",
    "embeddings = []\n",
    "for chunk in tqdm(range(0, len(df), chunk_size)):\n",
    "    print(f\"Processing chunk {chunk}\")\n",
    "    path = '/media/saplab/MinhNVMe/relahash/temp/embeddings_{}.pt'.format(chunk)\n",
    "    print(path)\n",
    "    embedding = torch.load(path)\n",
    "    embeddings.append(embedding[\"embedding\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = torch.cat(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1347836, 768])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = faiss.IndexFlatL2(768)\n",
    "index.add(embeddings.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "faiss.write_index(index, '/home/saplab/thaiminhpv/EFISS/AI-model-query-service/index/4.0.0/index.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1347836"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.ntotal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "path = \"/home/saplab/thaiminhpv/EFISS/AI-model-query-service/trainer/notebooks/test.jpg\"\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device('cpu')\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "def inference(path: str):\n",
    "    img = cv2.imread(path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    # img = torch.Tensor(img).to(device)\n",
    "    inputs = processor(images=img, return_tensors=\"pt\")\n",
    "    inputs['pixel_values'] = inputs['pixel_values'].to(device)\n",
    "    print(inputs['pixel_values'].shape)\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "    print(logits.shape)\n",
    "    D, I = index.search(logits.cpu().numpy(), 10)     # actual search\n",
    "\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.imshow(img)\n",
    "    plt.title(\"Query image: \" + path)\n",
    "    plt.draw()\n",
    "\n",
    "    plt.figure(figsize=(20, 20))\n",
    "    plt.title(\"Result images\")\n",
    "    for i, e in enumerate(I[0]):\n",
    "        plt.subplot(1, 10, i + 1)\n",
    "        file_path = df['img_path'].iloc[e]\n",
    "        print(file_path)\n",
    "        img = cv2.imread(file_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        plt.imshow(img)\n",
    "    plt.draw()\n",
    "inference(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thaiminhpv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
