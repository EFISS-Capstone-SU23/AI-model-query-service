{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/minhpvt/Workspace/Code/FU-Lab/AI-model-query-service\n"
     ]
    }
   ],
   "source": [
    "%cd ../.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "This script uses YOLOv8 to offline crop images and wraps it into a HuggingFace IterableDataset\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import urllib\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "from datasets import Dataset, IterableDataset\n",
    "from ultralytics import YOLO\n",
    "import torch\n",
    "from transformers import ViTImageProcessor, ViTForImageClassification, ViTFeatureExtractor\n",
    "import torch.nn as nn\n",
    "\n",
    "import datasets\n",
    "datasets.disable_caching()\n",
    "\n",
    "# from torch.multiprocessing import Pool, Process, set_start_method\n",
    "# try:\n",
    "#      set_start_method('spawn')\n",
    "# except RuntimeError:\n",
    "#     pass\n",
    "\n",
    "TOTAL_SHARD = 24\n",
    "SHARD_ID = 0\n",
    "\n",
    "# Define your YOLOv8-related functions here\n",
    "def initialize_yolov8_model():\n",
    "    model = YOLO('torchscripts_models/yolo/yolov8n_12ep_24-7_32.5mAP.pt')\n",
    "    # model.to('cuda:0')\n",
    "    return model\n",
    "\n",
    "def crop_image_with_yolov8(model: YOLO, img: np.ndarray) -> list[np.ndarray]:\n",
    "    # YOLOv8 cropping logic here\n",
    "    result = model.predict(\n",
    "        source=img,\n",
    "        conf=0.3,\n",
    "        # device='0',\n",
    "        save=False,\n",
    "        verbose=False\n",
    "    )[0]\n",
    "    # Crop and return images\n",
    "    cropped_images = []\n",
    "    for box in result.boxes.xyxy:\n",
    "        x, y, _x, _y = list(box.int())\n",
    "        cropped_images.append(result.orig_img[y:_y, x:_x])\n",
    "    return cropped_images\n",
    "\n",
    "output_dir = 'data/product_images/'\n",
    "\n",
    "def read_img_from_network(image_path: str) -> np.ndarray:\n",
    "    # assert image_path.startswith('https://'), f\"Image path {image_path} is not a valid URL\"\n",
    "    prefix = 'https://data.efiss.tech/'\n",
    "    image_path = prefix + image_path\n",
    "    # image_path = 'https://images.unsplash.com/photo-1515886657613-9f3515b0c78f?ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D&auto=format&fit=crop&w=1920&q=80'\n",
    "    req = urllib.request.urlopen(image_path)\n",
    "    arr = np.asarray(bytearray(req.read()), dtype=np.uint8)\n",
    "    img = cv2.imdecode(arr, cv2.IMREAD_COLOR)\n",
    "    return img\n",
    "\n",
    "model = initialize_yolov8_model()\n",
    "def crop_image_to_multiple_images(row) -> dict:\n",
    "    image_path = row['img_path']\n",
    "    img = read_img_from_network(row['img_path'])\n",
    "    img_name = image_path.split(\"/\")[-1].split(\".\")[0]\n",
    "\n",
    "    try:\n",
    "        cropped_images = crop_image_with_yolov8(model, img)\n",
    "    except Exception as e:\n",
    "        print(f\"Error cropping image {image_path}: {e}\")\n",
    "        cropped_images = []\n",
    "\n",
    "    cropped_image_paths = []\n",
    "    for i, cropped_image in enumerate(cropped_images):\n",
    "        cropped_image_path = os.path.join(output_dir, f\"{img_name}_crop{i}.jpg\")\n",
    "        # cv2.imwrite(cropped_image_path, cropped_image)\n",
    "        cropped_image_paths.append(cropped_image_path)\n",
    "\n",
    "    # return {'cropped_img_paths': cropped_image_paths, 'cropped_images': cropped_images}\n",
    "    print(f\"Finished cropping {image_path}: {len(cropped_images)} images\")\n",
    "    row['cropped_img_paths'] = cropped_image_paths\n",
    "    row['cropped_images'] = cropped_images\n",
    "    return row\n",
    "\n",
    "# row.keys()\n",
    "# row['cropped_img_paths']\n",
    "import matplotlib.pyplot as plt\n",
    "def plot_img(img: np.ndarray):\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    plt.imshow(img)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ellipsis"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# plot_img(row['cropped_images'][0])\n",
    "processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224')\n",
    "ranking_model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224')\n",
    "ranking_model.classifier = nn.Identity()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "ranking_model.eval()\n",
    "ranking_model.to(device)\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1347836/1347836 [00:00<00:00, 3766359.42it/s]\n"
     ]
    }
   ],
   "source": [
    "to_be_index: list[str] = []\n",
    "# with open('database_info.txt', 'r') as f:\n",
    "#     for line in tqdm(f.readlines()):\n",
    "#         to_be_index.append(line.strip())\n",
    "# read from http\n",
    "data_path = 'https://data.efiss.tech/data/efiss/product_images.txt'\n",
    "import requests\n",
    "r = requests.get(data_path, stream=True)\n",
    "for line in tqdm(r.iter_lines()):\n",
    "    to_be_index.append(line.decode('utf-8').strip())\n",
    "\n",
    "# Convert to HuggingFace dataset\n",
    "dataset = Dataset.from_pandas(pd.DataFrame({'img_path': to_be_index}))\n",
    "dataset\n",
    "dataset = dataset.shard(num_shards=TOTAL_SHARD, index=SHARD_ID)\n",
    "# dataset = Dataset.from_dict(dataset[:34])\n",
    "dataset\n",
    "total_len = len(dataset)\n",
    "dataset = dataset.to_iterable_dataset()\n",
    "cropped_images = dataset.map(crop_image_to_multiple_images, batched=False, remove_columns=['img_path'])\n",
    "cropped_images\n",
    "# tokenize\n",
    "def tokenize_function(row):\n",
    "    imgs: list[np.ndarray] = row[\"cropped_images\"][0]  # batch size 1\n",
    "    out = None\n",
    "    for img in imgs:\n",
    "        img = np.array(img, dtype=np.uint8)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        inputs = processor(images=img, return_tensors=\"pt\")\n",
    "        inputs['pixel_values'] = [inputs['pixel_values'].squeeze()]\n",
    "        if not out:\n",
    "            out = inputs\n",
    "        else:\n",
    "            out['pixel_values'].append(inputs['pixel_values'][0])\n",
    "    if out:\n",
    "        out['cropped_img_paths'] = row['cropped_img_paths'][0] or []\n",
    "        return out\n",
    "    else:\n",
    "        return {'pixel_values': [], 'cropped_img_paths': []}\n",
    "        \n",
    "tokenized_images = cropped_images.map(tokenize_function, batched=True, batch_size=1, remove_columns=['cropped_images'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_images = tokenized_images.with_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = torch.utils.data.DataLoader(tokenized_images, batch_size=16, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/56160 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished cropping data/shopee_crop_yolo/images/6460ab0252e365505c04bdd6_0_shopee_thedelia_crop0.jpg: 2 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/minhpvt/anaconda3/envs/thaiminhpv-3.11/lib/python3.11/site-packages/datasets/formatting/torch_formatter.py:68: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(value, **{**default_dtype, **self.torch_tensor_kwargs})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished cropping data/shopee_crop_yolo/images/6460ab0252e365505c04bdd6_3_shopee_thedelia_crop2.jpg: 2 images\n",
      "Finished cropping data/shopee_crop_yolo/images/6460aafe52e365505c04bda5_2_shopee_22decembrestore_crop2.jpg: 2 images\n",
      "Finished cropping data/shopee_crop_yolo/images/6460ab0552e365505c04be03_1_shopee_thedelia_crop0.jpg: 2 images\n",
      "Finished cropping data/shopee_crop_yolo/images/6460ab0552e365505c04bdf7_1_shopee_thedelia_crop1.jpg: 2 images\n",
      "Finished cropping data/shopee_crop_yolo/images/6460aae452e365505c04bcb2_1_shopee_22decembrestore_crop1.jpg: 2 images\n",
      "Finished cropping data/shopee_crop_yolo/images/6460aae452e365505c04bcb2_7_shopee_22decembrestore_crop1.jpg: 2 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/56160 [00:04<72:16:03,  4.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished cropping data/shopee_crop_yolo/images/6460aae352e365505c04bca6_0_shopee_22decembrestore_crop0.jpg: 2 images\n",
      "Finished cropping data/shopee_crop_yolo/images/6460ab0352e365505c04bddf_2_shopee_thedelia_crop1.jpg: 2 images\n",
      "Finished cropping data/shopee_crop_yolo/images/6460aae152e365505c04bc91_0_shopee_22decembrestore_crop6.jpg: 2 images\n",
      "Finished cropping data/shopee_crop_yolo/images/6460ab0452e365505c04bdf1_2_shopee_thedelia_crop2.jpg: 2 images\n",
      "Finished cropping data/shopee_crop_yolo/images/6460ab0252e365505c04bdcd_2_shopee_thedelia_crop0.jpg: 2 images\n",
      "Finished cropping data/shopee_crop_yolo/images/6460aafb52e365505c04bd84_0_shopee_22decembrestore_crop1.jpg: 2 images\n",
      "Finished cropping data/shopee_crop_yolo/images/6460aaeb52e365505c04bceb_4_shopee_22decembrestore_crop0.jpg: 2 images\n",
      "Finished cropping data/shopee_crop_yolo/images/6460aaef52e365505c04bd0f_5_shopee_22decembrestore_crop2.jpg: 2 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/56160 [00:07<123:44:17,  7.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished cropping data/shopee_crop_yolo/images/6460aae652e365505c04bcc1_0_shopee_22decembrestore_crop2.jpg: 2 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['cropped_img_paths', 'pixel_values'])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_cropped_img_paths: list[str] = []\n",
    "out_embeddings: list[torch.Tensor] = []\n",
    "with torch.no_grad():\n",
    "    for i, row in enumerate(tqdm(dataloader, total=total_len)):\n",
    "        logits = ranking_model(pixel_values=row['pixel_values']).logits  # (batch_size, 768)\n",
    "\n",
    "        out_cropped_img_paths.extend(row['cropped_img_paths'])  # list[str]\n",
    "        out_embeddings.append(logits.cpu())\n",
    "embeddings = torch.cat(out_embeddings, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {\n",
    "    'shard_id': SHARD_ID,\n",
    "    'embeddings': embeddings.cpu().numpy(),\n",
    "    'cropped_img_paths': out_cropped_img_paths\n",
    "}\n",
    "import pickle\n",
    "import requests\n",
    "# prepare to send via http\n",
    "dumped_payload: bytes = pickle.dumps(payload)\n",
    "# send\n",
    "url = 'https://indexer.efiss.tech/result'\n",
    "r = requests.post(url, files={'file': ('file.pkl', dumped_payload, 'application/octet-stream')})\n",
    "print(r.status_code, r.reason)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
